{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41750ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../openbookqa/answer_reason_generated_question/'\n",
    "llmjudge_path = '../openbookqa/LLMjudge/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6560bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(main_path)\n",
    "if '.DS_Store' in files:\n",
    "    files.remove('.DS_Store')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e24cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # remove articles\n",
    "    text = re.sub(r\"\\b(the|a|an)\\b\", \"\", text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23040a9c",
   "metadata": {},
   "source": [
    "1. Is the reasoning logically valid and coherent? (yes/no)\"\n",
    "2. Does the reasoning support the model's answer? (yes/no)\"\n",
    "3. Is the final answer correct? (yes/no)\"\n",
    "4. Final verdict: (choose one)\"\n",
    "                \n",
    "- A. Correct answer and faithful reasoning\"\n",
    "- B. Correct answer but unfaithful or shallow reasoning\"\n",
    "- C. Wrong answer but reasonable attempt\"\n",
    "- D. Wrong answer and unfaithful reasoning\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_analysis(df):\n",
    "    \n",
    "    a = 0\n",
    "    correct = 0\n",
    "    for i, row in df.iterrows():\n",
    "        s = row['evaluation']\n",
    "        \n",
    "        try:\n",
    "            evaluation = ast.literal_eval(s)\n",
    "        except:\n",
    "            evaluation = [x.strip(\"'\\\"\") for x in s]\n",
    "            #evaluation = [item.strip() for item in s.strip('[]').split(',')]\n",
    "        if evaluation[2].lower() == 'yes':\n",
    "            correct += 1\n",
    "        if evaluation[1].lower() == 'yes':\n",
    "            a += 1\n",
    "    print(\"Correct answer and faithful reasoning\", a)\n",
    "    print(\"Correct answer \", correct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5450167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_emb(question_dict):\n",
    "    encoded_input = tokenizer(question_dict, padding=True, truncation=False, return_tensors='pt')\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df520cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(llmjudge_path + files[0]) as f:\n",
    "    d = json.load(f)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(main_path + files[162]) as f:\n",
    "    m = json.load(f)\n",
    "m.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "t = [x for x in m['generated_question'] if x != '']\n",
    "\n",
    "t[5]\n",
    "\n",
    "if m['question_num'][-1] == 0:\n",
    "    index = 1\n",
    "elif m['question_num'][-1] == 1:\n",
    "    index = 3\n",
    "if m['question_num'][-1] == 2:\n",
    "    index = 5\n",
    "t[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fa3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def clean_answer(x):\n",
    "    # If it's a list in string form like \"['A']\"\n",
    "    if isinstance(x, str) and x.startswith(\"[\") and x.endswith(\"]\"):\n",
    "        try:\n",
    "            x = ast.literal_eval(x)   # safely convert string -> list\n",
    "            if isinstance(x, list) and len(x) > 0:\n",
    "                return str(x[0]).strip(\"'\\\"\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Remove extra quotes if present (like \"'A'\")\n",
    "    if isinstance(x, str):\n",
    "        return x.strip(\"[]'\\\" \").strip()\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(files, main_path, llmjudge_path):\n",
    "    \n",
    "    merged = pd.DataFrame(columns = ['question', 'choices', 'answer', 'reason', 'GT_answer', 'generated_question', 'evaluation', 'question_num', 'q', 'ques_similarity_value', 'pred_reason', 'pred_answer'])\n",
    "    question = []\n",
    "    choices = []\n",
    "    answer = []\n",
    "    reason = []\n",
    "    GT_answer = []\n",
    "    generated_question = []\n",
    "    evaluation = []\n",
    "    question_num = []\n",
    "    ques_similarity_value = []\n",
    "    pred_reason = []\n",
    "    pred_answer = []\n",
    "    q = []\n",
    "    s = []\n",
    "    verdict_for_gen = []\n",
    "    verdict_for_gen_val = []\n",
    "    real_value = []\n",
    "    cos_sim = []\n",
    "    \n",
    "    for file_i, file_name in enumerate(files):\n",
    "        \n",
    "        with open(main_path + file_name, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        question.append(data['question'])\n",
    "        choices.append(data['choices'])\n",
    "        answer.append(data['answer'])\n",
    "        reason.append(data['reason'])\n",
    "        GT_answer.append(data['GT_answer'])\n",
    "        evaluation.append(data['evaluation'])\n",
    "        question_num.append(data['question_num'])\n",
    "        q.append(data['question_num'][-1])\n",
    "        s.append(data['ques_similarity_value'][-1])\n",
    "        ques_similarity_value.append(data['ques_similarity_value'])\n",
    "        pred_reason.append(data['pred_reason'])\n",
    "        pred_answer.append(data['pred_answer'])\n",
    "        \n",
    "        t = [x for x in data['generated_question'] if x != '']\n",
    "        generated_question.append(t)\n",
    "        \n",
    "        if data['GT_answer'] == 'A':\n",
    "            gt_value = data['choices']['text'][0]\n",
    "        elif data['GT_answer'] == 'B':\n",
    "            gt_value = data['choices']['text'][1]\n",
    "        elif data['GT_answer'] == 'C':\n",
    "            gt_value = data['choices']['text'][2]\n",
    "        elif data['GT_answer'] == 'D':\n",
    "            gt_value = data['choices']['text'][3]\n",
    "        \n",
    "        if data['question_num'][-1] == 0:\n",
    "            index = 1\n",
    "        elif data['question_num'][-1] == 1:\n",
    "            index = 3\n",
    "        if data['question_num'][-1] == 2:\n",
    "            index = 5\n",
    "        options_for_gen = t[index].split('\":')[1].split(',')\n",
    "        \n",
    "        newt = [x for x in options_for_gen if x != '']\n",
    "        clean_alphabet = str(clean_answer(data['pred_answer']))\n",
    "        \n",
    "        if len(newt) < 2:\n",
    "            all_options = newt[0].split('\\\\')\n",
    "            if clean_alphabet == 'A':\n",
    "                select = all_options[0]\n",
    "            elif clean_alphabet == 'B':\n",
    "                select = all_options[1]\n",
    "            elif clean_alphabet == 'C':\n",
    "                select = all_options[2]\n",
    "            elif clean_alphabet == 'D':\n",
    "                select = all_options[3]\n",
    "        else:\n",
    "            if clean_alphabet == 'A':\n",
    "                select = newt[0]\n",
    "            elif clean_alphabet == 'B':\n",
    "                select = newt[1]\n",
    "            elif clean_alphabet == 'C':\n",
    "                select = newt[2]\n",
    "            elif clean_alphabet == 'D':\n",
    "                select = newt[3]\n",
    "            \n",
    "            \n",
    "        temp = [gt_value, select]\n",
    "        sentence_embeddings = get_emb(temp)\n",
    "        \n",
    "        cs = cosine_similarity(sentence_embeddings[0].cpu().numpy().reshape(1, -1), sentence_embeddings[1].cpu().numpy().reshape(1, -1))[0][0]\n",
    "        if cs >= 0.5:\n",
    "            cos_sim.append(cs)\n",
    "            verdict_for_gen.append(True)\n",
    "            verdict_for_gen_val.append(select)\n",
    "            real_value.append(gt_value)\n",
    "        else:\n",
    "            cos_sim.append(cs)\n",
    "            verdict_for_gen.append(False)\n",
    "            verdict_for_gen_val.append(select)\n",
    "            real_value.append(gt_value)\n",
    "       \n",
    "        #print(options_for_gen, gt_value, select, cs)\n",
    "    \n",
    "    merged['question'] = question\n",
    "    merged['choices'] = choices\n",
    "    merged['answer'] = answer\n",
    "    merged['reason'] = reason\n",
    "    merged['GT_answer'] = GT_answer\n",
    "    merged['generated_question'] = generated_question\n",
    "    merged['evaluation'] = evaluation\n",
    "    merged['question_num'] = question_num\n",
    "    merged['q'] = q\n",
    "    merged['s'] = s\n",
    "    merged['ques_similarity_value'] = ques_similarity_value\n",
    "    merged['pred_reason'] = pred_reason\n",
    "    merged['pred_answer'] = pred_answer\n",
    "    merged['verdict_for_gen'] = verdict_for_gen\n",
    "    merged['verdict_for_gen_val'] = verdict_for_gen_val\n",
    "    merged['real_value'] = real_value\n",
    "    merged['cos_sim'] = cos_sim\n",
    "    \n",
    "    merged['answer'] = merged['answer'].apply(clean_answer)    \n",
    "    merged['gt_norm'] = merged['GT_answer'].apply(normalize)\n",
    "    merged['pred_norm'] = merged['answer'].apply(normalize)\n",
    "    \n",
    "    merged[\"match\"] = merged[\"gt_norm\"] == merged[\"pred_norm\"]\n",
    "    correct_rows = merged.loc[merged['match']==True]['match'].shape[0]\n",
    "    false_rows = merged.loc[merged['match']==False]['match'].shape[0]\n",
    "\n",
    "    print('1. Check true preds and wrong preds w.r.t GT')\n",
    "    print('correct answers ', correct_rows)\n",
    "    print('false answers ', false_rows)\n",
    "    \n",
    "    merged[\"pred_match\"] = merged[\"match\"] == merged[\"verdict_for_gen\"]\n",
    "    \n",
    "    pred_correct_rows = merged.loc[merged['pred_match']==True]['pred_match'].shape[0]\n",
    "    pred_false_rows = merged.loc[merged['pred_match']==False]['pred_match'].shape[0]\n",
    "    print('2. Check generated true preds and wrong preds w.r.t GT')\n",
    "    print('correct answers from generated ques', pred_correct_rows)\n",
    "    print('false answers from generated ques', pred_false_rows)\n",
    "    \n",
    "    merged[\"same_answer\"] = merged[\"match\"] == merged[\"pred_match\"]\n",
    "    pred_correct_rows_generated = merged.loc[merged['same_answer']==True]['same_answer'].shape[0]\n",
    "    pred_false_rows_generated = merged.loc[merged['same_answer']==False]['same_answer'].shape[0]\n",
    "    print('3. Find how many are same and how many are changed')\n",
    "    print('common correct between correct answers and generated answers', pred_correct_rows_generated)\n",
    "    print('common false between correct answers and generated answers', pred_false_rows_generated)\n",
    "    \n",
    "    print('4. For all - finding LLMJudge')\n",
    "    evaluation_analysis(merged)\n",
    "    \n",
    "    print('6. How many questions with q0, q1, q2')\n",
    "    q2 = merged.loc[merged['q']==2]\n",
    "    q1 = merged.loc[merged['q']==1]\n",
    "    q0 = merged.loc[merged['q']==0]\n",
    "    print([q0.shape[0], q1.shape[0], q2.shape[0]])\n",
    "        \n",
    "    reason_sim = []\n",
    "    temp = []\n",
    "    for j, row in merged.iterrows():\n",
    "        temp = []\n",
    "        re = row['reason']\n",
    "        str_re = ' '.join(re) \n",
    "        gen_re = row['pred_reason']\n",
    "        str_gen_re = ' '.join(gen_re)\n",
    "        temp = [str_re, str_gen_re]\n",
    "        sentence_embeddings = get_emb(temp)\n",
    "        similarity = cosine_similarity(sentence_embeddings[0].cpu().numpy().reshape(1, -1), sentence_embeddings[1].cpu().numpy().reshape(1, -1))[0][0]\n",
    "        reason_sim.append(similarity)\n",
    "    merged['reason_sim'] = reason_sim\n",
    "    \n",
    "    \n",
    "    print('7. similarity between reasons and generated reasons based on questions')\n",
    "    simq0 = merged.loc[(merged['same_answer'] == True) & (merged['q'] == 0) ]['reason_sim']\n",
    "    simq1 = merged.loc[(merged['same_answer'] == True) & (merged['q'] == 1) ]['reason_sim']\n",
    "    simq2 = merged.loc[(merged['same_answer'] == True) & (merged['q'] == 2) ]['reason_sim']\n",
    "    print(simq0.shape[0], simq1.shape[0], simq2.shape[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(simq0.max(), simq0.min())    \n",
    "    print(simq1.max(), simq1.min())    \n",
    "    print(simq2.max(), simq2.min())\n",
    "    \n",
    "    print('8. similarity between reasons and generated reasons based on questions')\n",
    "    simq0 = merged.loc[(merged['same_answer'] == False) & (merged['q'] == 0) ]['reason_sim']\n",
    "    simq1 = merged.loc[(merged['same_answer'] == False) & (merged['q'] == 1) ]['reason_sim']\n",
    "    simq2 = merged.loc[(merged['same_answer'] == False) & (merged['q'] == 2) ]['reason_sim']\n",
    "    print(simq0.shape[0], simq1.shape[0], simq2.shape[0])\n",
    "    print(simq0.max(), simq0.min())    \n",
    "    print(simq1.max(), simq1.min())    \n",
    "    print(simq2.max(), simq2.min())\n",
    "    \n",
    "    simq0 = merged.loc[(merged['reason_sim'] > 0.2)]\n",
    "    \n",
    "    return merged\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4259d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis(files, main_path, llmjudge_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd39c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simq0 = df.loc[(df['same_answer'] == True)][['reason_sim', 's']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshols = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "counts = []\n",
    "for thre in threshols:\n",
    "    counts.append(simq0.loc[(simq0['reason_sim'] <= thre ) & (simq0['s'] <= thre )].shape[0])\n",
    "    \n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ecdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [str(t) for t in threshols]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.bar(xlabels, counts, width=0.8)\n",
    "\n",
    "ax.set_xlabel('Similarity Threshold')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Cumulative counts vs. similarity threshold')\n",
    "\n",
    "# optional: show values on top of bars\n",
    "y_max = max(counts)\n",
    "for i, v in enumerate(counts):\n",
    "    ax.text(i, v + 0.02*y_max, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/obqa_joint_similarity_bins.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12146ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['pred_match']==True) & (df['same_answer']==True)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b712e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['match']==False]\n",
    "row = a.iloc[67]\n",
    "for each in p:\n",
    "    print(each, row[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51891157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['same_answer'] == True) & (df['reason_sim'] >=0.5) & (df['s'] >=0.5) ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc07f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "simq0 = df.loc[(df['same_answer'] == True) & (df['q'] == 0) ]\n",
    "simq1 = df.loc[(df['same_answer'] == True) & (df['q'] == 1) ]\n",
    "simq2 = df.loc[(df['same_answer'] == True) & (df['q'] == 2) ]\n",
    "\n",
    "simq3 = df.loc[(df['same_answer'] == False) & (df['q'] == 0) ]\n",
    "simq4 = df.loc[(df['same_answer'] == False) & (df['q'] == 1) ]\n",
    "simq5 = df.loc[(df['same_answer'] == False) & (df['q'] == 2) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ce8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simq0 = simq0.reset_index(drop = True)\n",
    "simq1 = simq1.reset_index(drop = True)\n",
    "simq2 = simq2.reset_index(drop = True)\n",
    "\n",
    "simq3 = simq3.reset_index(drop = True)\n",
    "simq4 = simq4.reset_index(drop = True)\n",
    "simq5 = simq5.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ['question',\n",
    " 'choices',\n",
    " 'answer',\n",
    " 'reason',\n",
    " 'GT_answer',\n",
    " 'generated_question',\n",
    " 'cos_sim',\n",
    " 'question_num',\n",
    " 'ques_similarity_value',\n",
    " 'pred_reason',\n",
    " 'pred_answer',\n",
    " 'reason_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = simq4.iloc[9]\n",
    "for each in p:\n",
    "    print(each, row[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7cc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
