{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb351bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../Tell2Design Data/'\n",
    "gt_path = '../Tell2Design Data/gt/'\n",
    "llmeval_path = '../Tell2Design Data/llmeval/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e24cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # remove articles\n",
    "    text = re.sub(r\"\\b(the|a|an)\\b\", \"\", text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23040a9c",
   "metadata": {},
   "source": [
    "1. Is the reasoning logically valid and coherent? (yes/no)\"\n",
    "2. Does the reasoning support the model's answer? (yes/no)\"\n",
    "3. Is the final answer correct? (yes/no)\"\n",
    "4. Final verdict: (choose one)\"\n",
    "                \n",
    "- A. Correct answer and faithful reasoning\"\n",
    "- B. Correct answer but unfaithful or shallow reasoning\"\n",
    "- C. Wrong answer but reasonable attempt\"\n",
    "- D. Wrong answer and unfaithful reasoning\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5450167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_emb(question_dict):\n",
    "    encoded_input = tokenizer(question_dict, padding=True, truncation=False, return_tensors='pt')\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fa3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_answer(x):\n",
    "    # If it's a list in string form like \"['A']\"\n",
    "    if isinstance(x, str) and x.startswith(\"[\") and x.endswith(\"]\"):\n",
    "        try:\n",
    "            x = ast.literal_eval(x)   # safely convert string -> list\n",
    "            if isinstance(x, list) and len(x) > 0:\n",
    "                return str(x[0]).strip(\"'\\\"\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Remove extra quotes if present (like \"'A'\")\n",
    "    if isinstance(x, str):\n",
    "        return x.strip(\"[]'\\\" \").strip()\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_steps(steps):\n",
    "    norm = []\n",
    "    for s in steps:\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'^\\s*\\d+\\.\\s*', '', s)         # remove leading \"1. \"\n",
    "        s = re.sub(r'\\(\\s*\\d+,\\s*\\d+,\\s*\\d+\\s*\\)', ' COLOR ', s)  # normalize rgb\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        norm.append(s)\n",
    "    return norm\n",
    "\n",
    "def soft_alignment_score(steps_a, steps_b, model_bert):\n",
    "    #model = SentenceTransformer(model_name)\n",
    "    A = normalize_steps(steps_a)\n",
    "    B = normalize_steps(steps_b)\n",
    "    \n",
    "    ea = model_bert.encode(A, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    eb = model_bert.encode(B, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim = util.cos_sim(ea, eb)  # |A| x |B|\n",
    "\n",
    "    # precision-like: each a finds best b\n",
    "    p = sim.max(dim=1).values.mean().item()\n",
    "    # recall-like: each b finds best a\n",
    "    r = sim.max(dim=0).values.mean().item()\n",
    "    return (p + r) / 2, p, r  # overall, precision-like, recall-like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_files = os.listdir(main_path + 'topological_ordering/')\n",
    "if '.DS_Store' in cent_files:\n",
    "    cent_files.remove('.DS_Store')\n",
    "len(cent_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gt_path  + cent_files[0]) as f:\n",
    "    gt = json.load(f)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gt_path  + cent_files[0]) as f:\n",
    "    gt = json.load(f)\n",
    "gt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(main_path + 'topological_ordering/' + cent_files[226]) as f:\n",
    "    d = json.load(f)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(llmeval_path + 'topological_ordering/' + cent_files[226]) as f:\n",
    "    d = json.load(f)\n",
    "d.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = ['centroid_distance', 'direct_adjacency', 'room_removal',\n",
    "               'common_neighbor', 'topological_ordering'\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "            elif '_' in gt_ans:\n",
    "                print(gt_ans)\n",
    "                gt_ans = gt_ans[:-2]\n",
    "                if 'living' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'master' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'common' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'dining' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                gt_answer.append(gt_ans)\n",
    "            '''\n",
    "elif '_' in gt_ans:\n",
    "    print(gt_ans)\n",
    "    temp = ast.literal_eval(gt_ans)\n",
    "    string = temp[0]\n",
    "    if '_' in string:\n",
    "        string = string[:-2]\n",
    "        if 'living' in string:\n",
    "            string += ' room'\n",
    "        if 'master' in string:\n",
    "            string += ' room'\n",
    "        if 'common' in string:\n",
    "            string += ' room'\n",
    "        if 'dining' in string:\n",
    "            string += ' room'\n",
    "        gt_answer.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(main_path, gt_path, llmeval_path, folder_name, gt_key):    \n",
    "    files = os.listdir(main_path + folder_name)\n",
    "    if '.DS_Store' in files:\n",
    "        files.remove('.DS_Store')\n",
    "\n",
    "    merged = pd.DataFrame(columns = ['gt_answer', 'pred_reason', 'pred_answer', 'gen_reason', 'gen_answer', 'pair0', 'pair1', 'num_rooms'])\n",
    "\n",
    "    gt_answer = []\n",
    "    num_rooms = []\n",
    "    pair0 = []\n",
    "    pair1 = []\n",
    "\n",
    "    pred_reason = []\n",
    "    pred_answer = []\n",
    "    \n",
    "    gen_reason = []\n",
    "    gen_answer = []\n",
    "    \n",
    "    correct_faithful = []\n",
    "    true_llm = []\n",
    "    \n",
    "    evaluation = []\n",
    "\n",
    "    for i, file_name in enumerate(files):\n",
    "        \n",
    "        with open(main_path + folder_name + '/' + file_name) as f:\n",
    "            pred = json.load(f)\n",
    "        with open(llmeval_path + folder_name + '/' + file_name) as f:\n",
    "            llm = json.load(f)\n",
    "        with open(gt_path  + file_name) as f:\n",
    "            gt = json.load(f)\n",
    "        with open(main_path + folder_name  + '_generated/' + file_name) as f:\n",
    "            gen = json.load(f)\n",
    "        \n",
    "        try:\n",
    "            gt_ans = str(gt[gt_key])\n",
    "            if gt_ans == 'absent':\n",
    "                gt_answer.append(gt_ans)\n",
    "            elif '_' in gt_ans:\n",
    "                print(gt_ans)\n",
    "                gt_ans = gt_ans[:-2]\n",
    "                if 'living' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'master' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'common' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                if 'dining' in gt_ans:\n",
    "                    gt_ans += ' room'\n",
    "                gt_answer.append(gt_ans)\n",
    "            else:\n",
    "                gt_answer.append(gt_ans)\n",
    "        except TypeError:\n",
    "            gt_answer.append(gt[gt_key])#.astype(str).str.lower()\n",
    "                    \n",
    "            \n",
    "            \n",
    "        pred_reason.append(pred['Reason'])\n",
    "        pred_answer.append(pred['Answer'][0])\n",
    "        pair0.append(pred['pair0'])\n",
    "        try:\n",
    "            pair1.append(pred['pair1'])\n",
    "        except KeyError:\n",
    "            pair1.append('-')\n",
    "        gen_reason.append(gen['Reason'])\n",
    "        gen_answer.append(gen['Answer'][0])\n",
    "        num_rooms.append(pred['num_rooms'])\n",
    "        \n",
    "        evaluation.append(llm['evaluation'])\n",
    "        try:\n",
    "            correct_faithful.append(llm['evaluation'][0])\n",
    "            true_llm.append(llm['evaluation'][-1])\n",
    "        except TypeError:\n",
    "            correct_faithful.append('A')\n",
    "            true_llm.append('yes')\n",
    "\n",
    "        \n",
    "    merged['gt_answer'] = gt_answer\n",
    "    merged['pred_reason'] = pred_reason\n",
    "    merged['pred_answer'] = pred_answer\n",
    "    merged['pair0'] = pair0\n",
    "    merged['pair1'] = pair1\n",
    "    merged['gen_reason'] = gen_reason\n",
    "    merged['gen_answer'] = gen_answer\n",
    "    merged['num_rooms'] = num_rooms\n",
    "    merged['correct_faithful'] = correct_faithful\n",
    "    merged['true_llm'] = true_llm\n",
    "    merged['evaluation'] = evaluation\n",
    "       \n",
    "    return merged\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f71054",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_centroid_distance = make_df(main_path, gt_path, llmeval_path, 'centroid_distance', 'close_centroid')\n",
    "df_direct_adjacency = make_df(main_path, gt_path, llmeval_path, 'direct_adjacency', 'direct_adjacency')\n",
    "#df_common_neighbor = make_df(main_path, gt_path, llmeval_path, 'common_neighbor', 'common_neighbor')\n",
    "#df_room_removal = make_df(main_path, gt_path, llmeval_path, 'room_removal', 'room_removal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(merged, model_bert):\n",
    "    merged['gt_answer'] = merged['gt_answer'].astype(str).str.lower()\n",
    "    merged['pred_answer'] = merged['pred_answer'].astype(str).str.lower()\n",
    "    merged['gen_answer'] = merged['gen_answer'].astype(str).str.lower()\n",
    "    \n",
    "    merged['gt_answer'] = merged['gt_answer'].apply(clean_answer)\n",
    "    merged['pred_answer'] = merged['pred_answer'].apply(clean_answer)   \n",
    "    merged['gen_answer'] = merged['gen_answer'].apply(clean_answer)   \n",
    "    \n",
    "    merged['gt_norm'] = merged['gt_answer'].apply(normalize)\n",
    "    merged['pred_norm'] = merged['pred_answer'].apply(normalize)\n",
    "    merged['gen_norm'] = merged['gen_answer'].apply(normalize)\n",
    "    \n",
    "    merged[\"match\"] = merged[\"pred_norm\"] == merged[\"gt_norm\"]\n",
    "    merged[\"pred_match\"] = merged[\"gt_norm\"] == merged[\"gen_norm\"]\n",
    "    merged[\"same_answer\"] = merged[\"match\"] == merged[\"pred_match\"] \n",
    "     \n",
    "    \n",
    "    correct_rows = merged.loc[merged['match']==True]['match'].shape[0]\n",
    "    false_rows = merged.loc[merged['match']==False]['match'].shape[0]\n",
    "\n",
    "    print('1. Check true preds and wrong preds w.r.t GT')\n",
    "    print('correct answers ', correct_rows)\n",
    "    print('false answers ', false_rows)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_correct_rows = merged.loc[merged['pred_match']==True]['pred_match'].shape[0]\n",
    "    pred_false_rows = merged.loc[merged['pred_match']==False]['pred_match'].shape[0]\n",
    "    print('2. Check generated true preds and wrong preds w.r.t GT')\n",
    "    print('correct answers from generated ques', pred_correct_rows)\n",
    "    print('false answers from generated ques', pred_false_rows)\n",
    "    \n",
    "    \n",
    "    pred_correct_rows_generated = merged.loc[merged['same_answer']==True]['same_answer'].shape[0]\n",
    "    pred_false_rows_generated = merged.loc[merged['same_answer']==False]['same_answer'].shape[0]\n",
    "    print('3. Find how many are same and how many are changed')\n",
    "    print('common correct between correct answers and generated answers', pred_correct_rows_generated)\n",
    "    print('common false between correct answers and generated answers', pred_false_rows_generated)\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416832ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroid_distance_1 = analysis(df_centroid_distance, model_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_direct_adjacency['pred_answer'] = df_direct_adjacency['pred_answer'].astype(str).str.lower()\n",
    "df_direct_adjacency['gen_answer'] = df_direct_adjacency['gen_answer'].astype(str).str.lower()\n",
    "\n",
    "df_direct_adjacency['pred_answer'] = df_direct_adjacency['pred_answer'].apply(clean_answer)   \n",
    "df_direct_adjacency['gen_answer'] = df_direct_adjacency['gen_answer'].apply(clean_answer)   \n",
    "\n",
    "df_direct_adjacency['pred_answer'] = df_direct_adjacency['pred_answer'].apply(normalize)\n",
    "df_direct_adjacency['gen_answer'] = df_direct_adjacency['gen_answer'].apply(normalize)\n",
    "    \n",
    "\n",
    "df_direct_adjacency[['pred_answer', 'gen_answer']] = df_direct_adjacency[['pred_answer', 'gen_answer']].apply(\n",
    "    lambda col: col.str.lower().replace({'yes': 'true', 'no': 'false'})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_direct_adjacency_1 = analysis(df_direct_adjacency, model_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_neighbor_1 = analysis(df_common_neighbor, model_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_room_removal_1 = analysis(df_room_removal, model_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e7464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14c6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
